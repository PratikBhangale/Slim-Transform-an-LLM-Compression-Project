{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"9\"\n",
    "\n",
    "token = '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'\n",
    "\n",
    "# Clear GPU Cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# Force garbage collection\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# Move models to GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "quantization_config_4bit = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters before pruning:\n",
      "model.embed_tokens.weight: 262668288 parameters\n",
      "model.layers.0.self_attn.q_proj.weight: 4194304 parameters\n",
      "model.layers.0.self_attn.k_proj.weight: 1048576 parameters\n",
      "model.layers.0.self_attn.v_proj.weight: 1048576 parameters\n",
      "model.layers.0.self_attn.o_proj.weight: 4194304 parameters\n",
      "model.layers.0.mlp.gate_proj.weight: 16777216 parameters\n",
      "model.layers.0.mlp.up_proj.weight: 16777216 parameters\n",
      "model.layers.0.mlp.down_proj.weight: 16777216 parameters\n",
      "model.layers.0.input_layernorm.weight: 2048 parameters\n",
      "model.layers.0.post_attention_layernorm.weight: 2048 parameters\n",
      "model.layers.1.self_attn.q_proj.weight: 4194304 parameters\n",
      "model.layers.1.self_attn.k_proj.weight: 1048576 parameters\n",
      "model.layers.1.self_attn.v_proj.weight: 1048576 parameters\n",
      "model.layers.1.self_attn.o_proj.weight: 4194304 parameters\n",
      "model.layers.1.mlp.gate_proj.weight: 16777216 parameters\n",
      "model.layers.1.mlp.up_proj.weight: 16777216 parameters\n",
      "model.layers.1.mlp.down_proj.weight: 16777216 parameters\n",
      "model.layers.1.input_layernorm.weight: 2048 parameters\n",
      "model.layers.1.post_attention_layernorm.weight: 2048 parameters\n",
      "model.layers.2.self_attn.q_proj.weight: 4194304 parameters\n",
      "model.layers.2.self_attn.k_proj.weight: 1048576 parameters\n",
      "model.layers.2.self_attn.v_proj.weight: 1048576 parameters\n",
      "model.layers.2.self_attn.o_proj.weight: 4194304 parameters\n",
      "model.layers.2.mlp.gate_proj.weight: 16777216 parameters\n",
      "model.layers.2.mlp.up_proj.weight: 16777216 parameters\n",
      "model.layers.2.mlp.down_proj.weight: 16777216 parameters\n",
      "model.layers.2.input_layernorm.weight: 2048 parameters\n",
      "model.layers.2.post_attention_layernorm.weight: 2048 parameters\n",
      "model.layers.3.self_attn.q_proj.weight: 4194304 parameters\n",
      "model.layers.3.self_attn.k_proj.weight: 1048576 parameters\n",
      "model.layers.3.self_attn.v_proj.weight: 1048576 parameters\n",
      "model.layers.3.self_attn.o_proj.weight: 4194304 parameters\n",
      "model.layers.3.mlp.gate_proj.weight: 16777216 parameters\n",
      "model.layers.3.mlp.up_proj.weight: 16777216 parameters\n",
      "model.layers.3.mlp.down_proj.weight: 16777216 parameters\n",
      "model.layers.3.input_layernorm.weight: 2048 parameters\n",
      "model.layers.3.post_attention_layernorm.weight: 2048 parameters\n",
      "model.layers.4.self_attn.q_proj.weight: 4194304 parameters\n",
      "model.layers.4.self_attn.k_proj.weight: 1048576 parameters\n",
      "model.layers.4.self_attn.v_proj.weight: 1048576 parameters\n",
      "model.layers.4.self_attn.o_proj.weight: 4194304 parameters\n",
      "model.layers.4.mlp.gate_proj.weight: 16777216 parameters\n",
      "model.layers.4.mlp.up_proj.weight: 16777216 parameters\n",
      "model.layers.4.mlp.down_proj.weight: 16777216 parameters\n",
      "model.layers.4.input_layernorm.weight: 2048 parameters\n",
      "model.layers.4.post_attention_layernorm.weight: 2048 parameters\n",
      "model.layers.5.self_attn.q_proj.weight: 4194304 parameters\n",
      "model.layers.5.self_attn.k_proj.weight: 1048576 parameters\n",
      "model.layers.5.self_attn.v_proj.weight: 1048576 parameters\n",
      "model.layers.5.self_attn.o_proj.weight: 4194304 parameters\n",
      "model.layers.5.mlp.gate_proj.weight: 16777216 parameters\n",
      "model.layers.5.mlp.up_proj.weight: 16777216 parameters\n",
      "model.layers.5.mlp.down_proj.weight: 16777216 parameters\n",
      "model.layers.5.input_layernorm.weight: 2048 parameters\n",
      "model.layers.5.post_attention_layernorm.weight: 2048 parameters\n",
      "model.layers.6.self_attn.q_proj.weight: 4194304 parameters\n",
      "model.layers.6.self_attn.k_proj.weight: 1048576 parameters\n",
      "model.layers.6.self_attn.v_proj.weight: 1048576 parameters\n",
      "model.layers.6.self_attn.o_proj.weight: 4194304 parameters\n",
      "model.layers.6.mlp.gate_proj.weight: 16777216 parameters\n",
      "model.layers.6.mlp.up_proj.weight: 16777216 parameters\n",
      "model.layers.6.mlp.down_proj.weight: 16777216 parameters\n",
      "model.layers.6.input_layernorm.weight: 2048 parameters\n",
      "model.layers.6.post_attention_layernorm.weight: 2048 parameters\n",
      "model.layers.7.self_attn.q_proj.weight: 4194304 parameters\n",
      "model.layers.7.self_attn.k_proj.weight: 1048576 parameters\n",
      "model.layers.7.self_attn.v_proj.weight: 1048576 parameters\n",
      "model.layers.7.self_attn.o_proj.weight: 4194304 parameters\n",
      "model.layers.7.mlp.gate_proj.weight: 16777216 parameters\n",
      "model.layers.7.mlp.up_proj.weight: 16777216 parameters\n",
      "model.layers.7.mlp.down_proj.weight: 16777216 parameters\n",
      "model.layers.7.input_layernorm.weight: 2048 parameters\n",
      "model.layers.7.post_attention_layernorm.weight: 2048 parameters\n",
      "model.layers.8.self_attn.q_proj.weight: 4194304 parameters\n",
      "model.layers.8.self_attn.k_proj.weight: 1048576 parameters\n",
      "model.layers.8.self_attn.v_proj.weight: 1048576 parameters\n",
      "model.layers.8.self_attn.o_proj.weight: 4194304 parameters\n",
      "model.layers.8.mlp.gate_proj.weight: 16777216 parameters\n",
      "model.layers.8.mlp.up_proj.weight: 16777216 parameters\n",
      "model.layers.8.mlp.down_proj.weight: 16777216 parameters\n",
      "model.layers.8.input_layernorm.weight: 2048 parameters\n",
      "model.layers.8.post_attention_layernorm.weight: 2048 parameters\n",
      "model.layers.9.self_attn.q_proj.weight: 4194304 parameters\n",
      "model.layers.9.self_attn.k_proj.weight: 1048576 parameters\n",
      "model.layers.9.self_attn.v_proj.weight: 1048576 parameters\n",
      "model.layers.9.self_attn.o_proj.weight: 4194304 parameters\n",
      "model.layers.9.mlp.gate_proj.weight: 16777216 parameters\n",
      "model.layers.9.mlp.up_proj.weight: 16777216 parameters\n",
      "model.layers.9.mlp.down_proj.weight: 16777216 parameters\n",
      "model.layers.9.input_layernorm.weight: 2048 parameters\n",
      "model.layers.9.post_attention_layernorm.weight: 2048 parameters\n",
      "model.layers.10.self_attn.q_proj.weight: 4194304 parameters\n",
      "model.layers.10.self_attn.k_proj.weight: 1048576 parameters\n",
      "model.layers.10.self_attn.v_proj.weight: 1048576 parameters\n",
      "model.layers.10.self_attn.o_proj.weight: 4194304 parameters\n",
      "model.layers.10.mlp.gate_proj.weight: 16777216 parameters\n",
      "model.layers.10.mlp.up_proj.weight: 16777216 parameters\n",
      "model.layers.10.mlp.down_proj.weight: 16777216 parameters\n",
      "model.layers.10.input_layernorm.weight: 2048 parameters\n",
      "model.layers.10.post_attention_layernorm.weight: 2048 parameters\n",
      "model.layers.11.self_attn.q_proj.weight: 4194304 parameters\n",
      "model.layers.11.self_attn.k_proj.weight: 1048576 parameters\n",
      "model.layers.11.self_attn.v_proj.weight: 1048576 parameters\n",
      "model.layers.11.self_attn.o_proj.weight: 4194304 parameters\n",
      "model.layers.11.mlp.gate_proj.weight: 16777216 parameters\n",
      "model.layers.11.mlp.up_proj.weight: 16777216 parameters\n",
      "model.layers.11.mlp.down_proj.weight: 16777216 parameters\n",
      "model.layers.11.input_layernorm.weight: 2048 parameters\n",
      "model.layers.11.post_attention_layernorm.weight: 2048 parameters\n",
      "model.layers.12.self_attn.q_proj.weight: 4194304 parameters\n",
      "model.layers.12.self_attn.k_proj.weight: 1048576 parameters\n",
      "model.layers.12.self_attn.v_proj.weight: 1048576 parameters\n",
      "model.layers.12.self_attn.o_proj.weight: 4194304 parameters\n",
      "model.layers.12.mlp.gate_proj.weight: 16777216 parameters\n",
      "model.layers.12.mlp.up_proj.weight: 16777216 parameters\n",
      "model.layers.12.mlp.down_proj.weight: 16777216 parameters\n",
      "model.layers.12.input_layernorm.weight: 2048 parameters\n",
      "model.layers.12.post_attention_layernorm.weight: 2048 parameters\n",
      "model.layers.13.self_attn.q_proj.weight: 4194304 parameters\n",
      "model.layers.13.self_attn.k_proj.weight: 1048576 parameters\n",
      "model.layers.13.self_attn.v_proj.weight: 1048576 parameters\n",
      "model.layers.13.self_attn.o_proj.weight: 4194304 parameters\n",
      "model.layers.13.mlp.gate_proj.weight: 16777216 parameters\n",
      "model.layers.13.mlp.up_proj.weight: 16777216 parameters\n",
      "model.layers.13.mlp.down_proj.weight: 16777216 parameters\n",
      "model.layers.13.input_layernorm.weight: 2048 parameters\n",
      "model.layers.13.post_attention_layernorm.weight: 2048 parameters\n",
      "model.layers.14.self_attn.q_proj.weight: 4194304 parameters\n",
      "model.layers.14.self_attn.k_proj.weight: 1048576 parameters\n",
      "model.layers.14.self_attn.v_proj.weight: 1048576 parameters\n",
      "model.layers.14.self_attn.o_proj.weight: 4194304 parameters\n",
      "model.layers.14.mlp.gate_proj.weight: 16777216 parameters\n",
      "model.layers.14.mlp.up_proj.weight: 16777216 parameters\n",
      "model.layers.14.mlp.down_proj.weight: 16777216 parameters\n",
      "model.layers.14.input_layernorm.weight: 2048 parameters\n",
      "model.layers.14.post_attention_layernorm.weight: 2048 parameters\n",
      "model.layers.15.self_attn.q_proj.weight: 4194304 parameters\n",
      "model.layers.15.self_attn.k_proj.weight: 1048576 parameters\n",
      "model.layers.15.self_attn.v_proj.weight: 1048576 parameters\n",
      "model.layers.15.self_attn.o_proj.weight: 4194304 parameters\n",
      "model.layers.15.mlp.gate_proj.weight: 16777216 parameters\n",
      "model.layers.15.mlp.up_proj.weight: 16777216 parameters\n",
      "model.layers.15.mlp.down_proj.weight: 16777216 parameters\n",
      "model.layers.15.input_layernorm.weight: 2048 parameters\n",
      "model.layers.15.post_attention_layernorm.weight: 2048 parameters\n",
      "model.norm.weight: 2048 parameters\n",
      "\n",
      "Model parameters after pruning:\n",
      "model.embed_tokens.weight: 183664846 active parameters\n",
      "model.layers.0.self_attn.q_proj.weight: 2934275 active parameters\n",
      "model.layers.0.self_attn.k_proj.weight: 733798 active parameters\n",
      "model.layers.0.self_attn.v_proj.weight: 733317 active parameters\n",
      "model.layers.0.self_attn.o_proj.weight: 2934243 active parameters\n",
      "model.layers.0.mlp.gate_proj.weight: 11728385 active parameters\n",
      "model.layers.0.mlp.up_proj.weight: 11736062 active parameters\n",
      "model.layers.0.mlp.down_proj.weight: 11736160 active parameters\n",
      "model.layers.0.input_layernorm.weight: 1415 active parameters\n",
      "model.layers.0.post_attention_layernorm.weight: 1334 active parameters\n",
      "model.layers.1.self_attn.q_proj.weight: 2935251 active parameters\n",
      "model.layers.1.self_attn.k_proj.weight: 733441 active parameters\n",
      "model.layers.1.self_attn.v_proj.weight: 733075 active parameters\n",
      "model.layers.1.self_attn.o_proj.weight: 2933770 active parameters\n",
      "model.layers.1.mlp.gate_proj.weight: 11729941 active parameters\n",
      "model.layers.1.mlp.up_proj.weight: 11728367 active parameters\n",
      "model.layers.1.mlp.down_proj.weight: 11734729 active parameters\n",
      "model.layers.1.input_layernorm.weight: 1407 active parameters\n",
      "model.layers.1.post_attention_layernorm.weight: 1431 active parameters\n",
      "model.layers.2.self_attn.q_proj.weight: 2931390 active parameters\n",
      "model.layers.2.self_attn.k_proj.weight: 733660 active parameters\n",
      "model.layers.2.self_attn.v_proj.weight: 732343 active parameters\n",
      "model.layers.2.self_attn.o_proj.weight: 2930426 active parameters\n",
      "model.layers.2.mlp.gate_proj.weight: 11739692 active parameters\n",
      "model.layers.2.mlp.up_proj.weight: 11743577 active parameters\n",
      "model.layers.2.mlp.down_proj.weight: 11743693 active parameters\n",
      "model.layers.2.input_layernorm.weight: 1390 active parameters\n",
      "model.layers.2.post_attention_layernorm.weight: 1329 active parameters\n",
      "model.layers.3.self_attn.q_proj.weight: 2930355 active parameters\n",
      "model.layers.3.self_attn.k_proj.weight: 733589 active parameters\n",
      "model.layers.3.self_attn.v_proj.weight: 733027 active parameters\n",
      "model.layers.3.self_attn.o_proj.weight: 2934540 active parameters\n",
      "model.layers.3.mlp.gate_proj.weight: 11730606 active parameters\n",
      "model.layers.3.mlp.up_proj.weight: 11732275 active parameters\n",
      "model.layers.3.mlp.down_proj.weight: 11730245 active parameters\n",
      "model.layers.3.input_layernorm.weight: 1403 active parameters\n",
      "model.layers.3.post_attention_layernorm.weight: 1387 active parameters\n",
      "model.layers.4.self_attn.q_proj.weight: 2933699 active parameters\n",
      "model.layers.4.self_attn.k_proj.weight: 733818 active parameters\n",
      "model.layers.4.self_attn.v_proj.weight: 732729 active parameters\n",
      "model.layers.4.self_attn.o_proj.weight: 2934040 active parameters\n",
      "model.layers.4.mlp.gate_proj.weight: 11718007 active parameters\n",
      "model.layers.4.mlp.up_proj.weight: 11741108 active parameters\n",
      "model.layers.4.mlp.down_proj.weight: 11720185 active parameters\n",
      "model.layers.4.input_layernorm.weight: 1411 active parameters\n",
      "model.layers.4.post_attention_layernorm.weight: 1374 active parameters\n",
      "model.layers.5.self_attn.q_proj.weight: 2935058 active parameters\n",
      "model.layers.5.self_attn.k_proj.weight: 733003 active parameters\n",
      "model.layers.5.self_attn.v_proj.weight: 733053 active parameters\n",
      "model.layers.5.self_attn.o_proj.weight: 2932681 active parameters\n",
      "model.layers.5.mlp.gate_proj.weight: 11740031 active parameters\n",
      "model.layers.5.mlp.up_proj.weight: 11731771 active parameters\n",
      "model.layers.5.mlp.down_proj.weight: 11723001 active parameters\n",
      "model.layers.5.input_layernorm.weight: 1412 active parameters\n",
      "model.layers.5.post_attention_layernorm.weight: 1416 active parameters\n",
      "model.layers.6.self_attn.q_proj.weight: 2931166 active parameters\n",
      "model.layers.6.self_attn.k_proj.weight: 733739 active parameters\n",
      "model.layers.6.self_attn.v_proj.weight: 733039 active parameters\n",
      "model.layers.6.self_attn.o_proj.weight: 2929531 active parameters\n",
      "model.layers.6.mlp.gate_proj.weight: 11729938 active parameters\n",
      "model.layers.6.mlp.up_proj.weight: 11732543 active parameters\n",
      "model.layers.6.mlp.down_proj.weight: 11734204 active parameters\n",
      "model.layers.6.input_layernorm.weight: 1420 active parameters\n",
      "model.layers.6.post_attention_layernorm.weight: 1416 active parameters\n",
      "model.layers.7.self_attn.q_proj.weight: 2928126 active parameters\n",
      "model.layers.7.self_attn.k_proj.weight: 733028 active parameters\n",
      "model.layers.7.self_attn.v_proj.weight: 732973 active parameters\n",
      "model.layers.7.self_attn.o_proj.weight: 2929519 active parameters\n",
      "model.layers.7.mlp.gate_proj.weight: 11725956 active parameters\n",
      "model.layers.7.mlp.up_proj.weight: 11730670 active parameters\n",
      "model.layers.7.mlp.down_proj.weight: 11728380 active parameters\n",
      "model.layers.7.input_layernorm.weight: 1394 active parameters\n",
      "model.layers.7.post_attention_layernorm.weight: 1413 active parameters\n",
      "model.layers.8.self_attn.q_proj.weight: 2934318 active parameters\n",
      "model.layers.8.self_attn.k_proj.weight: 733204 active parameters\n",
      "model.layers.8.self_attn.v_proj.weight: 733630 active parameters\n",
      "model.layers.8.self_attn.o_proj.weight: 2933784 active parameters\n",
      "model.layers.8.mlp.gate_proj.weight: 11725669 active parameters\n",
      "model.layers.8.mlp.up_proj.weight: 11733586 active parameters\n",
      "model.layers.8.mlp.down_proj.weight: 11721847 active parameters\n",
      "model.layers.8.input_layernorm.weight: 1432 active parameters\n",
      "model.layers.8.post_attention_layernorm.weight: 1324 active parameters\n",
      "model.layers.9.self_attn.q_proj.weight: 2933682 active parameters\n",
      "model.layers.9.self_attn.k_proj.weight: 733898 active parameters\n",
      "model.layers.9.self_attn.v_proj.weight: 733049 active parameters\n",
      "model.layers.9.self_attn.o_proj.weight: 2934024 active parameters\n",
      "model.layers.9.mlp.gate_proj.weight: 11732590 active parameters\n",
      "model.layers.9.mlp.up_proj.weight: 11726508 active parameters\n",
      "model.layers.9.mlp.down_proj.weight: 11743494 active parameters\n",
      "model.layers.9.input_layernorm.weight: 1416 active parameters\n",
      "model.layers.9.post_attention_layernorm.weight: 1300 active parameters\n",
      "model.layers.10.self_attn.q_proj.weight: 2933630 active parameters\n",
      "model.layers.10.self_attn.k_proj.weight: 732863 active parameters\n",
      "model.layers.10.self_attn.v_proj.weight: 733189 active parameters\n",
      "model.layers.10.self_attn.o_proj.weight: 2933645 active parameters\n",
      "model.layers.10.mlp.gate_proj.weight: 11726831 active parameters\n",
      "model.layers.10.mlp.up_proj.weight: 11731834 active parameters\n",
      "model.layers.10.mlp.down_proj.weight: 11730947 active parameters\n",
      "model.layers.10.input_layernorm.weight: 1393 active parameters\n",
      "model.layers.10.post_attention_layernorm.weight: 1397 active parameters\n",
      "model.layers.11.self_attn.q_proj.weight: 2932713 active parameters\n",
      "model.layers.11.self_attn.k_proj.weight: 733285 active parameters\n",
      "model.layers.11.self_attn.v_proj.weight: 732334 active parameters\n",
      "model.layers.11.self_attn.o_proj.weight: 2933987 active parameters\n",
      "model.layers.11.mlp.gate_proj.weight: 11739556 active parameters\n",
      "model.layers.11.mlp.up_proj.weight: 11742810 active parameters\n",
      "model.layers.11.mlp.down_proj.weight: 11734248 active parameters\n",
      "model.layers.11.input_layernorm.weight: 1413 active parameters\n",
      "model.layers.11.post_attention_layernorm.weight: 1419 active parameters\n",
      "model.layers.12.self_attn.q_proj.weight: 2934568 active parameters\n",
      "model.layers.12.self_attn.k_proj.weight: 733951 active parameters\n",
      "model.layers.12.self_attn.v_proj.weight: 732375 active parameters\n",
      "model.layers.12.self_attn.o_proj.weight: 2931692 active parameters\n",
      "model.layers.12.mlp.gate_proj.weight: 11739038 active parameters\n",
      "model.layers.12.mlp.up_proj.weight: 11737436 active parameters\n",
      "model.layers.12.mlp.down_proj.weight: 11733343 active parameters\n",
      "model.layers.12.input_layernorm.weight: 1412 active parameters\n",
      "model.layers.12.post_attention_layernorm.weight: 1382 active parameters\n",
      "model.layers.13.self_attn.q_proj.weight: 2935373 active parameters\n",
      "model.layers.13.self_attn.k_proj.weight: 734002 active parameters\n",
      "model.layers.13.self_attn.v_proj.weight: 733572 active parameters\n",
      "model.layers.13.self_attn.o_proj.weight: 2934365 active parameters\n",
      "model.layers.13.mlp.gate_proj.weight: 11740908 active parameters\n",
      "model.layers.13.mlp.up_proj.weight: 11723149 active parameters\n",
      "model.layers.13.mlp.down_proj.weight: 11725973 active parameters\n",
      "model.layers.13.input_layernorm.weight: 1430 active parameters\n",
      "model.layers.13.post_attention_layernorm.weight: 1376 active parameters\n",
      "model.layers.14.self_attn.q_proj.weight: 2934756 active parameters\n",
      "model.layers.14.self_attn.k_proj.weight: 732362 active parameters\n",
      "model.layers.14.self_attn.v_proj.weight: 733484 active parameters\n",
      "model.layers.14.self_attn.o_proj.weight: 2933130 active parameters\n",
      "model.layers.14.mlp.gate_proj.weight: 11726022 active parameters\n",
      "model.layers.14.mlp.up_proj.weight: 11726098 active parameters\n",
      "model.layers.14.mlp.down_proj.weight: 11727588 active parameters\n",
      "model.layers.14.input_layernorm.weight: 1412 active parameters\n",
      "model.layers.14.post_attention_layernorm.weight: 1414 active parameters\n",
      "model.layers.15.self_attn.q_proj.weight: 2933989 active parameters\n",
      "model.layers.15.self_attn.k_proj.weight: 733896 active parameters\n",
      "model.layers.15.self_attn.v_proj.weight: 733255 active parameters\n",
      "model.layers.15.self_attn.o_proj.weight: 2934987 active parameters\n",
      "model.layers.15.mlp.gate_proj.weight: 11734519 active parameters\n",
      "model.layers.15.mlp.up_proj.weight: 11735697 active parameters\n",
      "model.layers.15.mlp.down_proj.weight: 11724762 active parameters\n",
      "model.layers.15.input_layernorm.weight: 1397 active parameters\n",
      "model.layers.15.post_attention_layernorm.weight: 1416 active parameters\n",
      "model.norm.weight: 1419 active parameters\n",
      "Pruned model saved to magnitude_pruned_model\n"
     ]
    }
   ],
   "source": [
    "def prune_model(model, pruning_percentage=0.2):\n",
    "    \"\"\"\n",
    "    Perform magnitude pruning on the model weights.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to prune.\n",
    "        pruning_percentage (float): Percentage of weights to prune.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: Pruned model.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'weight' in name and param.requires_grad:\n",
    "                # Flatten weights \n",
    "                weights = param.data.cpu().numpy()\n",
    "                threshold = np.percentile(np.abs(weights), pruning_percentage * 100)\n",
    "\n",
    "                # Create a mask \n",
    "                mask = np.abs(weights) > threshold\n",
    "\n",
    "                # Apply the mask\n",
    "                pruned_weights = torch.from_numpy(weights * mask).to(param.device)\n",
    "                param.data.copy_(pruned_weights)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Load your model and tokenizer\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=token)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Display model parameters before pruning\n",
    "print(\"Model parameters before pruning:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.numel()} parameters\")\n",
    "\n",
    "# Perform magnitude pruning\n",
    "pruning_percentage = 0.3 \n",
    "model = prune_model(model, pruning_percentage)\n",
    "\n",
    "# Display model parameters after pruning\n",
    "print(\"\\nModel parameters after pruning:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.nonzero().size(0)} active parameters\")\n",
    "\n",
    "# Save the pruned model\n",
    "pruned_model_path = \"magnitude_pruned_model\"\n",
    "model.save_pretrained(pruned_model_path)\n",
    "tokenizer.save_pretrained(pruned_model_path)\n",
    "\n",
    "print(f\"Pruned model saved to {pruned_model_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FINETUNE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
